\section{Related Work}
\label{sec:Related-Work}

The field of Affective Computing has picked up steam in the last couple of decades. 
This field of study is interdisciplinary which culminates Computer Science, Computer Engineering, Physcology, and 
Physiology. With the average employee spending 7 hours a day on the computer, 6 at work and 1 at home, 
it is important to monitor the health conditions of users during thier time behind a screen \cite{computer-time}.

Affective computing can be defined as the study and development of systems and devices that can recognize, interpret, process, and simulate human affects. 
While the origins of the field may be traced as far back as to early philosophical inquiries into emotion, the more modern branch of 
computer science originated with Rosalind Picard's 1995 paper on affective computing.  A motivation for the research is the ability to simulate empathy. 
The machine should interpret the emotional state of humans and adapt its behavior to them, giving an appropriate response for those emotions. 
Affective computing technologies sense the emotional state of a user(via sensors, microphone, cameras and/or software logic) and respond by 
performing specific, predefined product/service features, such as changing a quiz or recommending a set of videos to fit the mood of the learner. 
The more humans rely on computer devices, the more crucial it is to want them to behave politely and be socially aware. Common-sense reasoning 
requires an understanding of the user’s emotional state.

One way to look at affective computing is human-computer interaction in which a device has the ability to detect and appropriately respond 
to its user’s emotions and other stimuli. A computing device with this capacity could gather clues to user emotion from a variety of sources. 
Facial expressions, posture, gestures, speech, the force or rhythm of key strokes and the temperature changes of the hand on a mouse can 
all signify changes in the user’s emotional state, and these can all be detected and interpreted by the same computer. A built-in camera, 
for example, can capture images of the user and use classification algorithms to process the datum that yields information about the user’s 
facial expression during computer usage. Speech recognition and gesture recognition are among the other technologies being explored for affective 
computing applications. Recognizing emotional information requires the extraction of meaningful patterns from the gathered data. 
This can be achieved through ML techniques that process different modalities like speech, natural languages, or facial expressions.

A major area in affective computing is the design of computational devices proposed to exhibit either innate emotional capabilities 
or that are capable of convincingly simulating emotions. A more practical approach, based on current technological capabilities, 
is the simulation of emotions in conversational agents in order to enrich and facilitate interactivity between humans and machines. 
While human emotions are often associated with surges in hormones, emotions in machines might be associated with abstract states 
associated with progress (or lack of progress) in autonomous learning systems. In this view, affective emotional states correspond to 
time-derivatives in the learning curve of an arbitrary learning system. Two major categories describing emotions in machine are: 
Emotional speech and Facial affect detection. Emotional speech includes: algorithms, databases and speech descriptors. 
The facial affect detection is based off of physiological monitoring. 

The process of speech/text affect detection requires the creation of a reliable database, knowledge base, or vector space model, 
broad enough to fit every need for its application, as well as the selection of a successful classifier which will allow for quick and 
accurate emotion identification. Currently, the most frequently used classifiers are linear discriminant classifiers (LDC), 
k-nearest neighbor (k-NN), Gaussian mixture model (GMM), support vector machines (SVM), artificial neural networks (ANN), decision 
tree algorithms and hidden Markov models (HMMs). Various studies have shown that choosing the appropriate classifier can significantly 
enhance the overall performance of the system. For this project, SVM is used to classify stress within male patients. The SVM model is 
a type of linear classifier which decides in which of the two or more possible classes, each input may fall into.

The vast majority of present systems are data-dependent. This creates one of the biggest challenges in detecting emotions based on speech, 
as it implicates choosing an appropriate database used to train the classifier. Most of the current accessible data was obtained from actors 
and is thus a representation of archetypal emotions. Those so-called acted databases are usually based on the Basic Emotions theory proposed 
by Paul Ekman, which assumes the existence of six basic emotions (anger, fear, disgust, surprise, joy, sadness), the others simply being a mix 
of the former ones. As with every computational practice, in affect detection by facial processing, some obstacles need to be surpassed, 
in order to fully unlock the hidden potential of the overall algorithm or method employed. The accuracy of modeling and tracking has been 
an issue, especially in the incipient stages of affective computing. As hardware evolves, new discoveries are made, and new practices are 
introduced, the lack of accuracy fades; leaving behind noise issues. However, methods for noise removal exist including neighborhood averaging, 
linear Gaussian smoothing, median filtering, outlier detection, etc.

Physiological monitoring can be used to detect a user’s emotional state by monitoring and analyzing their physiological signs. 
These signs range from their pulse and heart rate to the minute contractions of the facial muscles. This area of research is still 
in relative infancy as there seems to be more of a drive towards affect recognition through facial inputs. The three main physiological 
signs that can be analyzed are blood volume pulse, galvanic skin response, and facial electromyography. It can be extremely difficult 
to ensure that the sensor shining an infra-red light that monitors reflected light is always pointing at the same extremity, especially 
seeing as subjects often stretch and readjust their position while using a computer. There are other factors which can affect one's 
blood volume pulse. As it is a measure of blood flow through the extremities, if the subject feels hot, or particularly cold, then their 
body may allow more, or less, blood to flow to the extremities, all of this regardless of the subject's emotional state. 
Affective Computing has a bright future due to its limitless capabilities of developing useful data applications that can help 
researchers find data in effective time tables. 

Along with these stress level ratings, in particular, comes error, bias, and variance in technology or simply, the subjects themselves.
The goal of modern Affective Computing lies within providing quality, accurate conclusions where other methods are not 
consistent in. Efforts in facial recognition that classify the affect of a human have been developing in recent 
years. The changes in muscles across the face lead to attributes that Charles Dawrin infered about universally about 
humans. These attributes have been consolidated to various class' that have been paired to classification algorithms 
for further analysis which resulted in 96\% accuracy for stationary images \cite{facial-recognition}. 

With the dependency of computers steadily increasing over time, the need for machines to interpret a user's affective state 
is important for their overall health. The contrary could result to ruined relationships or even shortness of life. 
This paper attempts to focus on stress detection during daily computer tasks to mitigate the negative effects of 
stress over time. 
